Here’s a clean, end-to-end breakdown of the app as a user journey with concrete steps, decision points, inputs, and outputs. I’ve grouped features into stages (Onboarding, Main Interaction, Post-Interaction), and called out the backend endpoints and data you’ll need so the flow is easy to implement and test.

# Onboarding / First-Run

### 1) Configure a scraping job

* **User actions**

  * Open “New Job” page.
  * Paste a PubMed query (e.g., `("atrial fibrillation"[tiab]) AND (malnutrition[tiab])`).
  * Optionally set: max results, date range, include PMC only, crawl external journals, concurrency (1–5), and save-to folder.
  * Click **Validate query** → **Start job**.
* **System actions**

  * Validate query syntax (basic checks; length, illegal chars).
  * Save job draft to DB.
* **Decision points**

  * **Is query empty/invalid?** → show inline error.
  * **Concurrency above limit?** → clamp to safe default.
* **Inputs**

  * PubMed query string; advanced options (booleans, numbers).
* **Outputs**

  * Created `job_id` + initial “Queued” status.

**API / UI**

* `POST /api/jobs` → returns `{ job_id }`
* Next.js route: `/jobs/new` with form + client validation.

---

# Main Interaction

## A) Run search & list candidates

### 2) Execute PubMed search

* **User actions**

  * Click **Run search** (or it auto-runs on job start).
* **System actions**

  * Call PubMed API to run the search (store query → get PMIDs).
  * For each PMID, fetch summary/links (PubMed page URL, PMC ID if present).
  * Persist preliminary records (one per PMID) with status `PENDING_URLS`.
* **Decision points**

  * **No results?** → Mark job `EMPTY`; show “0 results” screen.
  * **Rate-limit/HTTP error?** → Retry with backoff; if persistent, mark job `FAILED_SEARCH`.
* **Inputs**

  * Query; optional date filters; max results.
* **Outputs**

  * List of articles: PMID, title, PubMed URL, PMC ID (if any), journal link (if resolvable), status.

**API / UI**

* `POST /api/jobs/{job_id}/search`
* UI shows a results table with: Title | PMID | Source | “PDF status”.

---

## B) Resolve PDF targets (PMC or external)

### 3) Determine best PDF source per article

* **User actions**

  * Toggle: **Prefer PMC** / **Allow external crawl**.
  * Click **Resolve sources** or continue (auto in pipeline).
* **System actions**

  * If PMC ID present → construct PMC PDF URL.
  * Else parse PubMed record’s “Full text links” to external journal.
  * Enqueue crawl tasks for external links if allowed.
* **Decision points**

  * **PMC available?** → Use PMC (typically direct & legal).
  * **External allowed by user?** If not, mark `SKIPPED_NO_PMC`.
  * **Robots.txt disallows?** → `SKIPPED_ROBOTS`.
  * **Paywalled / 403** → `FAILED_PAYWALL`.
* **Inputs**

  * PMC ID; external full-text URLs; crawler setting.
* **Outputs**

  * Chosen target URL per article; per-record `RESOLVED`/`SKIPPED`/`FAILED_*`.

**API / UI**

* `POST /api/jobs/{job_id}/resolve`
* UI chips show source decision: **PMC**, **External**, **Skipped**.

---

## C) Crawl (when needed) & detect PDF link

### 4) Crawl external journal pages (optional)

* **User actions**

  * Nothing beyond enabling “Allow external crawl.”
  * Can open the per-article drawer → **Preview crawl** to see detected links.
* **System actions**

  * Fetch HTML → follow canonical/full-text anchors → detect PDF links by patterns: `href*.pdf`, “Download PDF”, meta tags, link rels.
  * Normalize relative URLs.
  * Handle JS-rendered pages via lightweight heuristics (avoid headless by default for MVP).
* **Decision points**

  * **Found direct PDF?** → proceed.
  * **Found landing page only?** → attempt next hop (depth 2 max).
  * **Blocked / cookie wall?** → `FAILED_BLOCKED`.
* **Inputs**

  * External URL; crawl depth limit; timeout; robots policy.
* **Outputs**

  * Final PDF URL or failure state.

**API / UI**

* `POST /api/articles/{article_id}/crawl`
* UI per-row shows crawl status + last attempted URL.

---

## D) Download PDFs

### 5) Download selected PDFs

* **User actions**

  * Click **Download all** or select rows → **Download selected**.
  * Optional: set file naming pattern (e.g., `{pmid}_{first_author}_{year}.pdf`).
* **System actions**

  * Stream download to storage; verify `content-type` and file size; compute checksum.
  * Persist path, size, MIME, checksum; update article status `DOWNLOADED`.
  * Respect concurrency & rate limits.
* **Decision points**

  * **MIME not `application/pdf`?** → try content sniffing; else `FAILED_NOT_PDF`.
  * **Duplicate checksum?** → mark as `DUPLICATE` and skip saving or dedupe to single file.
  * **Disk quota exceeded?** → halt job `FAILED_STORAGE`.
* **Inputs**

  * Final PDF URL; naming template; concurrency setting.
* **Outputs**

  * Saved PDFs; per-article `DOWNLOADED`/`FAILED_*` statuses; error logs.

**API / UI**

* `POST /api/jobs/{job_id}/download`
* Progress bar with counts: Queued / Downloading / Done / Failed.

---

# Post-Interaction

## E) Review, export, and resume

### 6) Review results

* **User actions**

  * Filter table by status (Downloaded, Skipped, Failed).
  * Click an article → **Open PDF** or **View source**.
  * For failures: **Retry** or **Edit source URL** → **Re-download**.
* **System actions**

  * Serve PDFs from storage; show logs for failures.
* **Decision points**

  * **Retry threshold exceeded?** → disable retry button until input changes.
* **Inputs**

  * User filters and optional corrected URLs.
* **Outputs**

  * Updated statuses, clearer error messages.

**API / UI**

* `GET /api/jobs/{job_id}/results`
* `POST /api/articles/{article_id}/retry`

---

### 7) Export metadata & files

* **User actions**

  * Click **Export CSV/JSON** (PMID, title, chosen source, file path, status).
  * Click **Download ZIP** of all PDFs (optional).
* **System actions**

  * Generate metadata file; bundle PDFs into a ZIP if requested.
* **Decision points**

  * **ZIP too large?** → stream chunked or suggest batch export.
* **Inputs**

  * Job selection, export format.
* **Outputs**

  * `metadata.csv`/`metadata.json`, optional `pdfs.zip`.

**API / UI**

* `GET /api/jobs/{job_id}/export?format=csv|json`
* `GET /api/jobs/{job_id}/export-zip`

---

### 8) Save job & schedule (optional future)

* **User actions**

  * Click **Save job** with preset options; toggle **Schedule** (e.g., weekly).
* **System actions**

  * Persist job template; create a scheduled task (cron/worker).
* **Decision points**

  * **Duplicate schedule?** → warn/merge.
* **Inputs**

  * Name, query, options, schedule rule.
* **Outputs**

  * Re-runnable job template; next run time.

**API / UI**

* `POST /api/job-templates`
* `POST /api/job-templates/{id}/schedule`

---

# Key User Intents → Features Mapping

* **One-off bulk download (MVP):**

  * New job → Search → Resolve sources → Download → Export.
* **PMC-only quick harvest:**

  * New job (PMC only) → Search → Auto-resolve PMC → Download.
* **Diagnostic crawl without download:**

  * New job (No downloads) → Search → Resolve → Crawl only → Export metadata.
* **Recovery:**

  * Open existing job → Filter Failed → Fix URL/Retry → Export.

---

# Inputs & Outputs Summary (per feature)

| Feature            | Inputs                                                             | Outputs                                           |
| ------------------ | ------------------------------------------------------------------ | ------------------------------------------------- |
| Create Job         | Query, options (max, dates, PMC only, external crawl, concurrency) | job_id, status=Queued                             |
| Run Search         | job_id                                                             | PMIDs list, article stubs, status per record      |
| Resolve PDF Source | job_id, PMC preference, external allowed                           | Target URL per article; `RESOLVED/FAILED/SKIPPED` |
| Crawl External     | article_id, depth, timeout                                         | Final PDF URL or failure reason                   |
| Download PDFs      | job_id or selection, naming pattern                                | Saved files; per-record `DOWNLOADED/FAILED_*`     |
| Review & Retry     | filters, corrected URLs                                            | Updated statuses, logs                            |
| Export             | format, include paths                                              | CSV/JSON, optional ZIP                            |

---

# Decision Points (the big ones)

1. **PMC present?** → Prefer PMC direct PDF.
2. **External crawling allowed?** → If yes, crawl; else skip external.
3. **Robots/paywall/cookie walls?** → Respect robots; mark failed gracefully.
4. **Is the link truly a PDF?** → Validate MIME/extension; reject HTML masquerading as PDF.
5. **Duplicates by checksum?** → Deduplicate transparently.
6. **Rate limits/timeouts?** → Backoff, bounded retries, then fail with reason.

---

# Minimal Data Model (SQLite)

* `jobs`
  `id, created_at, name, query, max_results, date_from, date_to, pmc_only BOOL, allow_external BOOL, concurrency INT, status, error_msg`
* `articles`
  `id, job_id FK, pmid, title, pubmed_url, pmc_id, external_url, chosen_pdf_url, status, failure_reason, retries INT`
* `downloads`
  `id, article_id FK, file_path, file_name, mime, size_bytes, checksum, downloaded_at`
* `logs`
  `id, job_id FK, article_id FK NULL, level, message, ts`

---

# Suggested API (FastAPI)

* `POST /api/jobs` (create job)
* `POST /api/jobs/{job_id}/search`
* `POST /api/jobs/{job_id}/resolve`
* `POST /api/jobs/{job_id}/download`
* `GET  /api/jobs/{job_id}/results`
* `POST /api/articles/{article_id}/crawl` (optional manual)
* `POST /api/articles/{article_id}/retry`
* `GET  /api/jobs/{job_id}/export`
* `GET  /api/jobs/{job_id}/export-zip`

Workers: background tasks for search/resolve/crawl/download with a small queue.

---

# Next.js Pages & Components (MVP)

* `/jobs/new` — form with validation; “Start job”.
* `/jobs/{job_id}` — 3-step progress header (Search → Resolve → Download), results table with statuses, filters, retry, export.
* Components: `JobForm`, `ResultsTable`, `ProgressBar`, `StatusChip`, `RetryButton`, `ExportMenu`.

---

# Guardrails & Niceties (you’ll thank yourself later)

* Respect `robots.txt` and publisher terms of use; default to PMC where possible.
* Backoff + jitter on all network calls; central timeout config.
* Clear, user-readable failure reasons (e.g., “Paywall detected”, “Robots disallow”, “MIME not PDF”).
* Deterministic filenames (safe ASCII), checksum for dedupe.
* Concurrency cap (e.g., 3) with queue; avoid hammering journals.
* Dry-run mode (resolve only, no downloads) for quick diagnostics.

---

This breakdown gives you a straight line from user intent to concrete actions, API calls, DB records, and UI states. A good next step is to stub the endpoints with fake data so you can wire the UI and validate the user flow before implementing crawling and downloads.



